{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNro2DMCECzXMRRVID3WvZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "711f265a474e494d9792c763e41bec62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_90f63dca848e459fbff0f85cc6a08081",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a0a8472b9c734dfd977a1927057496ab",
              "IPY_MODEL_806c1634963c429b89ce8837904c2fa4"
            ]
          }
        },
        "90f63dca848e459fbff0f85cc6a08081": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0a8472b9c734dfd977a1927057496ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_125ad2c14e334cf595e0522cfdfcc85b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c2d47fb3afed4e9bb3c53f333c0f9287"
          }
        },
        "806c1634963c429b89ce8837904c2fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e258ef25faea4b729c9078f86f247257",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:30&lt;00:00, 12274934.36it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a154b6e8b6584a81976c095bfd3e39ed"
          }
        },
        "125ad2c14e334cf595e0522cfdfcc85b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c2d47fb3afed4e9bb3c53f333c0f9287": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e258ef25faea4b729c9078f86f247257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a154b6e8b6584a81976c095bfd3e39ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pandian-raja/EVA4_Session10/blob/master/Session_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV0SBUiKKtPu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "66b401f1-50b6-48b5-ec5c-30a25f7090bf"
      },
      "source": [
        "!pip install albumentations\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.6/dist-packages (0.1.12)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\r\u001b[K     |▌                               | 10kB 32.0MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 34.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 41.4MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 43.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 36.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 40.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 28.9MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 30.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92kB 27.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 112kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 143kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 153kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 174kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 184kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 204kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 225kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 235kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 245kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 256kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 266kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 286kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 307kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 327kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 337kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 348kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 358kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 368kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 378kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 389kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 399kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 409kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 419kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 430kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 440kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 450kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 460kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 471kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 481kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 491kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 501kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 512kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 522kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 532kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 542kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 552kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 563kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 573kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 583kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 593kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 604kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 614kB 26.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 624kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 26.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from albumentations) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.18.2)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.12.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.0.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (4.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (46.0.0)\n",
            "Building wheels for collected packages: imgaug\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=2780f660f05450dd44c6fe5c499d490ce25259f263bd365ab13162abc8859934\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built imgaug\n",
            "Installing collected packages: imgaug\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed imgaug-0.2.6\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tufDpTiwK82p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "52287503-cb9a-4c99-c100-81747bfa785e"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/pandian-raja/EVA4_Session8/master/resnet.py\n",
        "!wget https://raw.githubusercontent.com/pandian-raja/EVA4_Session9/master/GetData.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-28 03:35:21--  https://raw.githubusercontent.com/pandian-raja/EVA4_Session8/master/resnet.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4005 (3.9K) [text/plain]\n",
            "Saving to: ‘resnet.py’\n",
            "\n",
            "\rresnet.py             0%[                    ]       0  --.-KB/s               \rresnet.py           100%[===================>]   3.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-28 03:35:21 (78.9 MB/s) - ‘resnet.py’ saved [4005/4005]\n",
            "\n",
            "--2020-03-28 03:35:24--  https://raw.githubusercontent.com/pandian-raja/EVA4_Session9/master/GetData.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3234 (3.2K) [text/plain]\n",
            "Saving to: ‘GetData.py’\n",
            "\n",
            "GetData.py          100%[===================>]   3.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-28 03:35:24 (20.0 MB/s) - ‘GetData.py’ saved [3234/3234]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1ZysBNQLFwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "class train_and_validate():\n",
        "    def train(trainloader, device, model,EPOCH):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
        "        for epoch in range(EPOCH):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(trainloader, 0):\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                # forward + backward + optimize\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "                if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
        "                    running_loss = 0.0\n",
        "\n",
        "        print('Finished Training')\n",
        "        return model\n",
        "\n",
        "\n",
        "    def validate(testloader, device, model):\n",
        "        dataiter = iter(testloader)\n",
        "        images, labels = dataiter.next()\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in testloader:\n",
        "                images, labels = data\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        print('Accuracy of the network on the 10000 test images: %2d %%' % ((100 * correct) / total))    \n",
        "\n",
        "  \n",
        "    def classValidation(testloader, device, model, classes):\n",
        "        class_correct = list(0. for i in range(10))\n",
        "        class_total = list(0. for i in range(10))\n",
        "        with torch.no_grad():\n",
        "            for data in testloader:\n",
        "                images, labels = data\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                c = (predicted == labels).squeeze()\n",
        "                for i in range(4):\n",
        "                    label = labels[i]\n",
        "                    class_correct[label] += c[i].item()\n",
        "                    class_total[label] += 1\n",
        "\n",
        "\n",
        "            for i in range(10):\n",
        "                print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMt7cKT_LLk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import resnet as rs\n",
        "# import train_and_validate as tv\n",
        "import GetData as gd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBexHLVjLOA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "711f265a474e494d9792c763e41bec62",
            "90f63dca848e459fbff0f85cc6a08081",
            "a0a8472b9c734dfd977a1927057496ab",
            "806c1634963c429b89ce8837904c2fa4",
            "125ad2c14e334cf595e0522cfdfcc85b",
            "c2d47fb3afed4e9bb3c53f333c0f9287",
            "e258ef25faea4b729c9078f86f247257",
            "a154b6e8b6584a81976c095bfd3e39ed"
          ]
        },
        "outputId": "827738d4-8371-4669-e414-16ec0d4b5071"
      },
      "source": [
        "trainloader, testloader, classes, device = gd.GetData.importDataset()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "711f265a474e494d9792c763e41bec62",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoWD8c3ELT6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = rs.ResNet18().to(device);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnmoT4FYLWG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ae92496-06d2-4cbb-b4a2-4b32cdeab315"
      },
      "source": [
        "summary(model, input_size=(3, 32, 32))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
            "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
            "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
            "           Conv2d-13          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-14          [-1, 128, 16, 16]             256\n",
            "           Conv2d-15          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
            "           Conv2d-17          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-18          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-19          [-1, 128, 16, 16]               0\n",
            "           Conv2d-20          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
            "           Conv2d-22          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-24          [-1, 128, 16, 16]               0\n",
            "           Conv2d-25            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-26            [-1, 256, 8, 8]             512\n",
            "           Conv2d-27            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-28            [-1, 256, 8, 8]             512\n",
            "           Conv2d-29            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-30            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-31            [-1, 256, 8, 8]               0\n",
            "           Conv2d-32            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-33            [-1, 256, 8, 8]             512\n",
            "           Conv2d-34            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-35            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-36            [-1, 256, 8, 8]               0\n",
            "           Conv2d-37            [-1, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-39            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-41            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-43            [-1, 512, 4, 4]               0\n",
            "           Conv2d-44            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-46            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-48            [-1, 512, 4, 4]               0\n",
            "           Linear-49                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,173,962\n",
            "Trainable params: 11,173,962\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 11.25\n",
            "Params size (MB): 42.63\n",
            "Estimated Total Size (MB): 53.89\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5-v9s5hLYeH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "18aeaf1f-8972-4c2c-975c-0869301cc250"
      },
      "source": [
        "model = train_and_validate.train(trainloader,device,model,4)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.088\n",
            "[1,  4000] loss: 1.851\n",
            "[1,  6000] loss: 1.710\n",
            "[1,  8000] loss: 1.583\n",
            "[1, 10000] loss: 1.505\n",
            "[1, 12000] loss: 1.396\n",
            "[2,  2000] loss: 1.272\n",
            "[2,  4000] loss: 1.240\n",
            "[2,  6000] loss: 1.214\n",
            "[2,  8000] loss: 1.165\n",
            "[2, 10000] loss: 1.116\n",
            "[2, 12000] loss: 1.109\n",
            "[3,  2000] loss: 1.024\n",
            "[3,  4000] loss: 1.011\n",
            "[3,  6000] loss: 0.975\n",
            "[3,  8000] loss: 0.980\n",
            "[3, 10000] loss: 0.939\n",
            "[3, 12000] loss: 0.926\n",
            "[4,  2000] loss: 0.876\n",
            "[4,  4000] loss: 0.873\n",
            "[4,  6000] loss: 0.862\n",
            "[4,  8000] loss: 0.862\n",
            "[4, 10000] loss: 0.852\n",
            "[4, 12000] loss: 0.826\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5AbmYB6LawM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "312ce870-7484-4126-9f27-a6e57a64d43f"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/1Konny/gradcam_plus_plus-pytorch/master/utils.py\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-28 03:49:16--  https://raw.githubusercontent.com/1Konny/gradcam_plus_plus-pytorch/master/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7831 (7.6K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   7.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-28 03:49:16 (129 MB/s) - ‘utils.py’ saved [7831/7831]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeY47B6ZLrsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def visualize_cam(mask, img):\n",
        "    \"\"\"Make heatmap from mask and synthesize GradCAM result image using heatmap and img.\n",
        "    Args:\n",
        "        mask (torch.tensor): mask shape of (1, 1, H, W) and each element has value in range [0, 1]\n",
        "        img (torch.tensor): img shape of (1, 3, H, W) and each pixel value is in range [0, 1]\n",
        "        \n",
        "    Return:\n",
        "        heatmap (torch.tensor): heatmap img shape of (3, H, W)\n",
        "        result (torch.tensor): synthesized GradCAM result of same shape with heatmap.\n",
        "    \"\"\"\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask.squeeze().cpu()), cv2.COLORMAP_JET)\n",
        "    heatmap = torch.from_numpy(heatmap).permute(2, 0, 1).float().div(255)\n",
        "    b, g, r = heatmap.split(1)\n",
        "    heatmap = torch.cat([r, g, b])\n",
        "    \n",
        "    result = heatmap+img.cpu()\n",
        "    result = result.div(result.max()).squeeze()\n",
        "    \n",
        "    return heatmap, result\n",
        "\n",
        "\n",
        "def find_resnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find resnet layer to calculate GradCAM and GradCAM++\n",
        "    \n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'conv1'\n",
        "            target_layer_name = 'layer1'\n",
        "            target_layer_name = 'layer1_basicblock0'\n",
        "            target_layer_name = 'layer1_basicblock0_relu'\n",
        "            target_layer_name = 'layer1_bottleneck0'\n",
        "            target_layer_name = 'layer1_bottleneck0_conv1'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample_0'\n",
        "            target_layer_name = 'avgpool'\n",
        "            target_layer_name = 'fc'\n",
        "            \n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if 'layer' in target_layer_name:\n",
        "        hierarchy = target_layer_name.split('_')\n",
        "        layer_num = int(hierarchy[0].lstrip('layer'))\n",
        "        if layer_num == 1:\n",
        "            target_layer = arch.layer1\n",
        "        elif layer_num == 2:\n",
        "            target_layer = arch.layer2\n",
        "        elif layer_num == 3:\n",
        "            target_layer = arch.layer3\n",
        "        elif layer_num == 4:\n",
        "            target_layer = arch.layer4\n",
        "        else:\n",
        "            raise ValueError('unknown layer : {}'.format(target_layer_name))\n",
        "\n",
        "        if len(hierarchy) >= 2:\n",
        "            bottleneck_num = int(hierarchy[1].lower().lstrip('bottleneck').lstrip('basicblock'))\n",
        "            target_layer = target_layer[bottleneck_num]\n",
        "\n",
        "        if len(hierarchy) >= 3:\n",
        "            target_layer = target_layer._modules[hierarchy[2]]\n",
        "                \n",
        "        if len(hierarchy) == 4:\n",
        "            target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    else:\n",
        "        target_layer = arch._modules[target_layer_name]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_densenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find densenet layer to calculate GradCAM and GradCAM++\n",
        "    \n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_transition1'\n",
        "            target_layer_name = 'features_transition1_norm'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'classifier'\n",
        "            \n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    \n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) >= 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    if len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_vgg_layer(arch, target_layer_name):\n",
        "    \"\"\"Find vgg layer to calculate GradCAM and GradCAM++\n",
        "    \n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_42'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "            \n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_alexnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find alexnet layer to calculate GradCAM and GradCAM++\n",
        "    \n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_0'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "            \n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_squeezenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n",
        "    \n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features_12'\n",
        "            target_layer_name = 'features_12_expand3x3'\n",
        "            target_layer_name = 'features_12_expand3x3_activation'\n",
        "            \n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2]+'_'+hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def denormalize(tensor, mean, std):\n",
        "    if not tensor.ndimension() == 4:\n",
        "        raise TypeError('tensor should be 4D')\n",
        "\n",
        "    mean = torch.FloatTensor(mean).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "    std = torch.FloatTensor(std).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "\n",
        "    return tensor.mul(std).add(mean)\n",
        "\n",
        "\n",
        "def normalize(tensor, mean, std):\n",
        "    if not tensor.ndimension() == 4:\n",
        "        raise TypeError('tensor should be 4D')\n",
        "\n",
        "    mean = torch.FloatTensor(mean).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "    std = torch.FloatTensor(std).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "\n",
        "    return tensor.sub(mean).div(std)\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return self.do(tensor)\n",
        "    \n",
        "    def do(self, tensor):\n",
        "        return normalize(tensor, self.mean, self.std)\n",
        "    \n",
        "    def undo(self, tensor):\n",
        "        return denormalize(tensor, self.mean, self.std)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fGIVv16TArq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from utils import find_alexnet_layer, find_vgg_layer, find_resnet_layer, find_densenet_layer, find_squeezenet_layer\n",
        "\n",
        "\n",
        "class GradCAM(object):\n",
        "    \"\"\"Calculate GradCAM salinecy map.\n",
        "\n",
        "    A simple example:\n",
        "\n",
        "        # initialize a model, model_dict and gradcam\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)\n",
        "        resnet.eval()\n",
        "        model_dict = dict(model_type='resnet', arch=resnet, layer_name='layer4', input_size=(224, 224))\n",
        "        gradcam = GradCAM(model_dict)\n",
        "\n",
        "        # get an image and normalize with mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
        "        img = load_img()\n",
        "        normed_img = normalizer(img)\n",
        "\n",
        "        # get a GradCAM saliency map on the class index 10.\n",
        "        mask, logit = gradcam(normed_img, class_idx=10)\n",
        "\n",
        "        # make heatmap from mask and synthesize saliency map using heatmap and img\n",
        "        heatmap, cam_result = visualize_cam(mask, img)\n",
        "\n",
        "\n",
        "    Args:\n",
        "        model_dict (dict): a dictionary that contains 'model_type', 'arch', layer_name', 'input_size'(optional) as keys.\n",
        "        verbose (bool): whether to print output size of the saliency map givien 'layer_name' and 'input_size' in model_dict.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_dict, verbose=False):\n",
        "        model_type = model_dict['type']\n",
        "        layer_name = model_dict['layer_name']\n",
        "        self.model_arch = model_dict['arch']\n",
        "\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        if 'vgg' in model_type.lower():\n",
        "            target_layer = find_vgg_layer(self.model_arch, layer_name)\n",
        "        elif 'resnet' in model_type.lower():\n",
        "            target_layer = find_resnet_layer(self.model_arch, layer_name)\n",
        "        elif 'densenet' in model_type.lower():\n",
        "            target_layer = find_densenet_layer(self.model_arch, layer_name)\n",
        "        elif 'alexnet' in model_type.lower():\n",
        "            target_layer = find_alexnet_layer(self.model_arch, layer_name)\n",
        "        elif 'squeezenet' in model_type.lower():\n",
        "            target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n",
        "\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        if verbose:\n",
        "            try:\n",
        "                input_size = model_dict['input_size']\n",
        "            except KeyError:\n",
        "                print(\"please specify size of input image in model_dict. e.g. {'input_size':(224, 224)}\")\n",
        "                pass\n",
        "            else:\n",
        "                device = 'cuda' if next(self.model_arch.parameters()).is_cuda else 'cpu'\n",
        "                self.model_arch(torch.zeros(1, 3, *(input_size), device=device))\n",
        "                print('saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: input image with shape of (1, 3, H, W)\n",
        "            class_idx (int): class index for calculating GradCAM.\n",
        "                    If not specified, the class index that makes the highest model prediction score will be used.\n",
        "        Return:\n",
        "            mask: saliency map of the same spatial dimension with input\n",
        "            logit: model output\n",
        "        \"\"\"\n",
        "        b, c, h, w = input.size()\n",
        "\n",
        "        logit = self.model_arch(input)\n",
        "        if class_idx is None:\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        #alpha = F.relu(gradients.view(b, k, -1)).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "\n",
        "        saliency_map = (weights*activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.upsample(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "\n",
        "        return saliency_map, logit\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "\n",
        "class GradCAMpp(GradCAM):\n",
        "    \"\"\"Calculate GradCAM++ salinecy map.\n",
        "\n",
        "    A simple example:\n",
        "\n",
        "        # initialize a model, model_dict and gradcampp\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)\n",
        "        resnet.eval()\n",
        "        model_dict = dict(model_type='resnet', arch=resnet, layer_name='layer4', input_size=(224, 224))\n",
        "        gradcampp = GradCAMpp(model_dict)\n",
        "\n",
        "        # get an image and normalize with mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
        "        img = load_img()\n",
        "        normed_img = normalizer(img)\n",
        "\n",
        "        # get a GradCAM saliency map on the class index 10.\n",
        "        mask, logit = gradcampp(normed_img, class_idx=10)\n",
        "\n",
        "        # make heatmap from mask and synthesize saliency map using heatmap and img\n",
        "        heatmap, cam_result = visualize_cam(mask, img)\n",
        "\n",
        "\n",
        "    Args:\n",
        "        model_dict (dict): a dictionary that contains 'model_type', 'arch', layer_name', 'input_size'(optional) as keys.\n",
        "        verbose (bool): whether to print output size of the saliency map givien 'layer_name' and 'input_size' in model_dict.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_dict, verbose=False):\n",
        "        super(GradCAMpp, self).__init__(model_dict, verbose)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: input image with shape of (1, 3, H, W)\n",
        "            class_idx (int): class index for calculating GradCAM.\n",
        "                    If not specified, the class index that makes the highest model prediction score will be used.\n",
        "        Return:\n",
        "            mask: saliency map of the same spatial dimension with input\n",
        "            logit: model output\n",
        "        \"\"\"\n",
        "        b, c, h, w = input.size()\n",
        "\n",
        "        logit = self.model_arch(input)\n",
        "        if class_idx is None:\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            score = logit[:, class_idx].squeeze() \n",
        "            \n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        gradients = self.gradients['value'] # dS/dA\n",
        "        activations = self.activations['value'] # A\n",
        "        b, k, u, v = gradients.size()\n",
        "\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "                activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "\n",
        "        alpha = alpha_num.div(alpha_denom+1e-7)\n",
        "        positive_gradients = F.relu(score.exp()*gradients) # ReLU(dY/dA) == ReLU(exp(S)*dS/dA))\n",
        "        weights = (alpha*positive_gradients).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        saliency_map = (weights*activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.upsample(saliency_map, size=(32, 32), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map-saliency_map_min).div(saliency_map_max-saliency_map_min).data\n",
        "\n",
        "        return saliency_map, logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH3-UZUsL1Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import visualize_cam, Normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlXKH1wQkPYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(dataloader):\n",
        "    running_corrects = 0\n",
        "    running_loss=0\n",
        "    pred = []\n",
        "    true = []\n",
        "    pred_wrong = []\n",
        "    true_wrong = []\n",
        "    image = []\n",
        "    sm = nn.Softmax(dim = 1)\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        data = data.type(torch.cuda.FloatTensor)\n",
        "        target = target.type(torch.cuda.LongTensor)\n",
        "        model.eval()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        output = sm(output)\n",
        "        _, preds = torch.max(output, 1)\n",
        "        running_corrects = running_corrects + torch.sum(preds == target.data)\n",
        "        running_loss += loss.item() * data.size(0)\n",
        "        preds = preds.cpu().numpy()\n",
        "        target = target.cpu().numpy()\n",
        "        preds = np.reshape(preds,(len(preds),1))\n",
        "        target = np.reshape(target,(len(preds),1))\n",
        "        data = data.cpu().numpy()\n",
        "        \n",
        "        for i in range(len(preds)):\n",
        "            pred.append(preds[i])\n",
        "            true.append(target[i])\n",
        "            if(preds[i]!=target[i]):\n",
        "                pred_wrong.append(preds[i])\n",
        "                true_wrong.append(target[i])\n",
        "                image.append(data[i])\n",
        "      \n",
        "    epoch_acc = running_corrects.double()/(len(dataloader)*batch_size)\n",
        "    epoch_loss = running_loss/(len(dataloader)*batch_size)\n",
        "    print(epoch_acc,epoch_loss)\n",
        "    return true,pred,image,true_wrong,pred_wrong"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EML2I2Emk2Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(testloader, device, model):\n",
        "        dataiter = iter(testloader)\n",
        "        images, labels = dataiter.next()\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in testloader:\n",
        "                images, labels = data\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        print('Accuracy of the network on the 10000 test images: %2d %%' % ((100 * correct) / total))    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFAFndPbkG_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrong_plot(true,ima,pred,encoder,inv_normalize,n_figures ):\n",
        "    print('Classes in order Actual and Predicted')\n",
        "    n_row = int(n_figures/5)\n",
        "    fig,axes = plt.subplots(figsize=(10, 10), nrows = n_row, ncols=5)\n",
        "    for ax in axes.flatten():\n",
        "        a = random.randint(0,len(true)-1)\n",
        "    \n",
        "        image,correct,wrong = ima[a],true[a],pred[a]\n",
        "        image = torch.from_numpy(image)\n",
        "        correct = int(correct)\n",
        "        c = encoder[correct]\n",
        "        wrong = int(wrong)\n",
        "        w = encoder[wrong]\n",
        "        f = 'A:'+str(c.item()) + ',' +'P:'+str(w.item())\n",
        "        f = 'A:'+str(correct) + ',' +'P:'+str(wrong)\n",
        "        # image = image.numpy().transpose(1,2,0)\n",
        "        image = image.squeeze()\n",
        "        im = ax.imshow(image, cmap='gray', interpolation='none')\n",
        "        ax.set_title(f)\n",
        "        ax.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hiXpbvGL4Ci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "3c74df83-736a-4ebf-806c-644d0e156d2d"
      },
      "source": [
        "model.eval(), model.cuda();\n",
        "normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "torch_img = torch.from_numpy(np.asarray(testset)).permute(2, 0, 1).unsqueeze(0).float().div(255).cpu().cuda()\n",
        "torch_img = F.upsample(torch_img, size=(32, 32), mode='bilinear', align_corners=False)\n",
        "normed_torch_img = normalizer(torch_img)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-21dbcab66d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtorch_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnormed_torch_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu9yuGIlL_QQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "75e51083-0318-4b17-b6b9-d4c8b29e2b4c"
      },
      "source": [
        "cam_dict = dict()\n",
        "resnet_model_dict = dict(type='resnet', arch=model, layer_name='layer4', input_size=(32, 32))\n",
        "resnet_gradcam = GradCAM(resnet_model_dict, True)\n",
        "resnet_gradcampp = GradCAMpp(resnet_model_dict, True)\n",
        "cam_dict['resnet'] = [resnet_gradcam, resnet_gradcampp]\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saliency_map size : torch.Size([4, 4])\n",
            "saliency_map size : torch.Size([4, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh1qSKK4UUkR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "830ac8dd-9996-41ce-db2a-bec8aad3219a"
      },
      "source": [
        "mask, _ = gradcam(normed_torch_img)\n",
        "heatmap, result = visualize_cam(mask, torch_img)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2416: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbEVHZWFh_wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from albumentations import Compose, RandomCrop, Normalize, HorizontalFlip, Cutout, VerticalFlip\n",
        "from albumentations.pytorch import ToTensor\n",
        "class albumCompose_test:\n",
        "    def __init__(self):\n",
        "        self.albumentations_transform = Compose({\n",
        "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        })\n",
        "    def __call__(self, img):\n",
        "        img = np.array(img)\n",
        "        img = self.albumentations_transform(image=img)['image']\n",
        "        img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n",
        "        return torch.tensor(img, dtype=torch.float)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FJQSazUhzhF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab9514c1-10d8-46d8-f781-d4a879743f1f"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "        download=True, transform=albumCompose_test())\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX_YTiN0RVHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import PIL\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import make_grid, save_image\n",
        "img_name = 'water-bird.JPEG'\n",
        "# img_path = os.path.join(img_dir, img_name)\n",
        "\n",
        "pil_img = PIL.Image.open(img_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sPkX1r8MDac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "66d392f5-1ef1-4b48-bf59-fbb6fa334055"
      },
      "source": [
        "images = []\n",
        "for gradcam, gradcam_pp in cam_dict.values():\n",
        "    mask, _ = gradcam(normed_torch_img)\n",
        "    heatmap, result = visualize_cam(mask, torch_img)\n",
        "\n",
        "    mask_pp, _ = gradcam_pp(normed_torch_img)\n",
        "    heatmap_pp, result_pp = visualize_cam(mask_pp, torch_img)\n",
        "    \n",
        "    images.append(torch.stack([torch_img.squeeze().cpu(), heatmap, heatmap_pp, result, result_pp], 0))\n",
        "   "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-b19b2ae4c9a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgradcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradcam_pp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcam_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualize_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-e6076af101ab>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input, class_idx, retain_graph)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-e6076af101ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, class_idx, retain_graph)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mlogit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_arch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPxHnan4eWnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "con = torch.cat((a, b), 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfUUKtuZUkRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = make_grid(torch.cat(images, 0), nrow=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwTki0FWTfRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch as t\n",
        "from torchvision.transforms import ToPILImage\n",
        "from IPython.display import Image\n",
        "to_img = ToPILImage()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYMhrD-BVSqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da1b06f4-0047-4389-9c89-dec41114b201"
      },
      "source": [
        "to_img(images)\n",
        "\n",
        "# display imagefile\n",
        "Image('/path/to/my.png')\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "/path/to/my.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlalEEG0R4zF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "937118be-1b4a-4152-a28c-8018136a2513"
      },
      "source": [
        "output_dir = 'outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_name = img_name\n",
        "output_path = os.path.join(output_dir, output_name)\n",
        "\n",
        "save_image(images, output_path)\n",
        "PIL.Image.open(output_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4e01ec662a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'outputs'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutput_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zisv61lSTVXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XKd2gnlTX0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}